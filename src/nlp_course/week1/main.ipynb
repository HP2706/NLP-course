{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from datasets import load_dataset\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "from typing import List, Union, Any\n",
    "from instructor import Instructor\n",
    "# various utils for distilling features \n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Awaitable\n",
    "import instructor\n",
    "import asyncio\n",
    "from typing import AsyncGenerator\n",
    "from tqdm.asyncio import tqdm as async_tqdm\n",
    "\n",
    "import nest_asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "ds_val = ds[\"validation\"].to_pandas()\n",
    "ds_train = ds[\"train\"].to_pandas()\n",
    "print(len(ds_train))\n",
    "print(len(ds_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TranslationResponse(BaseModel):\n",
    "    original_text : str = Field(description=\"The original text that was translated\")\n",
    "    translated_text : str = Field(description=\"The translated text into English from either Finnish, Japanese or Russian\")\n",
    "    \n",
    "async def embed_chunk(chunk : List[str]) -> Awaitable[List[float]]:\n",
    "    client = AsyncOpenAI()\n",
    "    response = client.embeddings.create(input=chunk, model='text-embedding-ada-002')\n",
    "    return [r.embedding for r in response.data]\n",
    "\n",
    "class TranslationResponse(BaseModel):\n",
    "    original_text : str = Field(description=\"The original text that was translated\")\n",
    "    translated_text : str = Field(description=\"The translated text into English from either Finnish, Japanese or Russian\")\n",
    "    \n",
    "async def embed_chunk(chunk : List[str]) -> Awaitable[List[float]]:\n",
    "    client = AsyncOpenAI()\n",
    "    response = client.embeddings.create(input=chunk, model='text-embedding-ada-002')\n",
    "    return [r.embedding for r in response.data]\n",
    "\n",
    "async def translate_chunk(\n",
    "    chunk: List[tuple[str, str]]\n",
    ") -> List[TranslationResponse]:\n",
    "    client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "    text_chunks = '\\n'.join([f'lang: {t[0]} \\n text: {t[1]}' for t in chunk])\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4-0125-preview\",\n",
    "        response_model=List[TranslationResponse],\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a translation assistant that translates text from Finnish, Japanese or Russian to English. Please output a list of TranslationResponse objects ordered by the original input list.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Translate the following text to English: {text_chunks}\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    assert len(response) == len(chunk), f\"Response length ({len(response)}) does not match chunk length ({len(chunk)})\"\n",
    "    return response\n",
    "\n",
    "async def translate_questions(df: pd.DataFrame):\n",
    "    batch_size = 5\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No rows to translate.\")\n",
    "        return df\n",
    "    \n",
    "    # Initialize the question_translated column for all rows\n",
    "    df[\"question_translated\"] = \"\"\n",
    "    \n",
    "    async def process_batch(task_id : int, batch) -> Awaitable[List[TranslationResponse]]:\n",
    "        translations = await translate_chunk(\n",
    "            [(row[\"lang\"], row[\"question\"]) for _, row in batch.iterrows()]\n",
    "        )\n",
    "        return task_id, translations\n",
    "    \n",
    "    \n",
    "    batches = [df.iloc[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
    "    \n",
    "    all_translations = []\n",
    "    \n",
    "    tasks = [process_batch(task_id=i, batch=batch) for i, batch in enumerate(batches)]\n",
    "    \n",
    "    async for translation in async_tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Translating\"):\n",
    "        all_translations.append(await translation)\n",
    "\n",
    "    all_translations.sort(key=lambda x: x[0])\n",
    "    \n",
    "    flattened_translations = [item for sublist in all_translations for item in sublist[1]]\n",
    "    # Update only the rows that needed translation\n",
    "    for i, translation in enumerate(flattened_translations):\n",
    "        df.loc[df.index[i], \"question_translated\"] = translation.translated_text\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Wrap the translation process in a try-except block\n",
    "try:\n",
    "    translated_df = asyncio.run(translate_questions(ds_train[:10]))\n",
    "    current_dir = os.getcwd()\n",
    "    if current_dir.endswith(\"week1\"):\n",
    "        os.chdir(\"..\")\n",
    "    else:\n",
    "        print(\"current dir\", current_dir)\n",
    "    os.makedirs(\"dataset\", exist_ok=True)\n",
    "    translated_df.to_parquet(\"dataset/translated_df_train.parquet\", index=False)\n",
    "    os.chdir(current_dir)\n",
    "    print(\"Translation completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during translation: {str(e)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [(_dict[\"lang\"], _dict[\"question\"]) for _dict in ds_train[['lang', 'question']].to_dict(orient='records')]\n",
    "print(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" import os\n",
    "import pandas as pd\n",
    "from pandasai import Agent \"\"\"\n",
    "\n",
    "print(ds_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize \n",
    "#we load the llama3 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_column(\n",
    "    df : pd.DataFrame, \n",
    "    tokenizer : AutoTokenizer,\n",
    "    tokenizer_name : str\n",
    "):\n",
    "    text_cols = ['question', 'context', 'answer']\n",
    "    for col in tqdm(text_cols):\n",
    "        df[f\"{col}_{tokenizer_name}_n_tokens\"] = df[col].apply(lambda x: len(tokenizer.encode(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize basic data statistics for train-\n",
    "#ing and validation data in each of the languages Finnish (fi), Japanese\n",
    "#(ja) and Russian (ru).\n",
    "\n",
    "# distribution of answerable and not answerable questions\n",
    "def create_language_plots(train_data, val_data, title_prefix):\n",
    "    # Create subplots\n",
    "    fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]],\n",
    "                        subplot_titles=(f\"{title_prefix} - Training Data\", \n",
    "                                        f\"{title_prefix} - Validation Data\"))\n",
    "\n",
    "    # Only fi, ja, ru\n",
    "    selected_langs = ['fi', 'ja', 'ru']\n",
    "    dfs = [train_data, val_data]\n",
    "    \n",
    "    figs = []\n",
    "    for df in dfs:\n",
    "        selected_data = df[df['lang'].isin(selected_langs)]\n",
    "        fig = px.sunburst(selected_data, path=['lang', 'answerable'],\n",
    "                       color='lang',\n",
    "                       color_discrete_map={True: 'rgba(0,100,0,0.7)', False: 'rgba(220,20,60,0.7)'})\n",
    "        fig.update_traces(textinfo='label+percent parent')\n",
    "        figs.append(fig)\n",
    "    \n",
    "    for fig in figs:\n",
    "        fig.add_trace(fig.data[0], row=1, col=1)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(title_text=f\"{title_prefix} Language Distribution\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def create_language_histogram(train_data, title_prefix):\n",
    "    selected_langs = ['fi', 'ja', 'ru']\n",
    "    df = train_data[train_data['lang'].isin(selected_langs)]\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=selected_langs)\n",
    "    \n",
    "    for i, lang in enumerate(selected_langs, 1):\n",
    "        lang_data = df[df['lang'] == lang]\n",
    "        \n",
    "        # Answerable questions (True)\n",
    "        answerable_data = lang_data[lang_data['answerable']]['question_n_tokens']\n",
    "        # Non-answerable questions (False)\n",
    "        non_answerable_data = lang_data[~lang_data['answerable']]['question_n_tokens']\n",
    "        \n",
    "        # Calculate bin edges for both distributions\n",
    "        bins = np.histogram_bin_edges(np.concatenate([answerable_data, non_answerable_data]), bins=50)\n",
    "        \n",
    "        # Compute histograms\n",
    "        answerable_hist, _ = np.histogram(answerable_data, bins=bins)\n",
    "        non_answerable_hist, _ = np.histogram(non_answerable_data, bins=bins)\n",
    "        \n",
    "        # Normalize histograms\n",
    "        answerable_hist = answerable_hist / answerable_hist.sum()\n",
    "        non_answerable_hist = non_answerable_hist / non_answerable_hist.sum()\n",
    "        \n",
    "        # Plot normalized histograms\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=bins[:-1], y=answerable_hist,\n",
    "                   name=f'{lang} - Answerable',\n",
    "                   marker_color='blue',\n",
    "                   opacity=0.7),\n",
    "            row=1, col=i\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=bins[:-1], y=non_answerable_hist,\n",
    "                   name=f'{lang} - Non-answerable',\n",
    "                   marker_color='red',\n",
    "                   opacity=0.7),\n",
    "            row=1, col=i\n",
    "        )\n",
    "        \n",
    "        # Update x-axis and y-axis labels\n",
    "        fig.update_xaxes(title_text=\"Number of Tokens\", row=1, col=i)\n",
    "        fig.update_yaxes(title_text=\"Relative Frequency\" if i == 1 else None, row=1, col=i)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=f\"{title_prefix} - Distribution of Question Token Counts by Language and Answerability\",\n",
    "        barmode='overlay',\n",
    "        height=500,\n",
    "        width=1200\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "# Create and display the plot\n",
    "fig_train = create_language_histogram(ds_train, \"Training Data\")\n",
    "fig_train.write_image(\"plots/week1_a_lang_token_distribution_normalized.png\")\n",
    "\n",
    "# Create and display plots for training data\n",
    "\"\"\" fig_train = create_language_plots(ds_train, ds_val, \"Training Data\")\n",
    "fig_train.write_image(\"plots/week1_a_dataset.png\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% (b) For each of the languages Finnish, Japanese and Russian, report the 5 most common \n",
    "#% words in the questions from the training set. What kind of words are they?\n",
    "\n",
    "#collect all tokens from the questions, and count them\n",
    "import MeCab\n",
    "\n",
    "def get_top_words(df: pd.DataFrame, lang: str, n=5):\n",
    "    df_lang = df[df['lang'] == lang].copy()\n",
    "    \n",
    "    if lang == 'ja':\n",
    "        mecab = MeCab.Tagger(\"-Owakati\")  # Initialize MeCab tokenizer\n",
    "        df_lang.loc[:, 'words_question_tokens'] = df_lang['question'].apply(lambda x: mecab.parse(x).split())\n",
    "    else:\n",
    "        df_lang.loc[:, 'words_question_tokens'] = df_lang['question'].apply(lambda x: x.split(' '))\n",
    "    \n",
    "    all_tokens = np.concatenate(df_lang['words_question_tokens'].values)\n",
    "    unique, counts = np.unique(all_tokens, return_counts=True)\n",
    "    sorted_indices = np.argsort(counts)[::-1]\n",
    "    top_unique_tokens = unique[sorted_indices][:n]\n",
    "    top_tokens_dict = {token: int(count) for token, count in zip(top_unique_tokens, counts[sorted_indices][:n])}\n",
    "    return top_tokens_dict\n",
    "\n",
    "def visualize_top_tokens(\n",
    "    df : pd.DataFrame, \n",
    "    lang : str, \n",
    "    n=5\n",
    "):\n",
    "    top_tokens = get_top_words(df, lang, n)\n",
    "    fig = px.bar(x=list(top_tokens.keys()), y=list(top_tokens.values()), labels={'x':'Tokens', 'y':'Counts'})\n",
    "    fig.update_layout(title=f\"Top {n} Tokens in {lang}\")\n",
    "    fig.show()\n",
    "    return fig, top_tokens\n",
    "\n",
    "for lang in ['fi', 'ja', 'ru']:\n",
    "    fig, top_tokens = visualize_top_tokens(ds_train, lang, 5)\n",
    "    fig.write_image(f\"plots/week1_b_top_5_tokens_{lang}.png\")\n",
    "    print(top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% (c) Implement a rule-based classifier that predicts whether a question is answerable \n",
    "#% or impossible, only using the document (context) and question. You may use machine \n",
    "#% translation as a component. Use the answerable field to evaluate it on the validation set. \n",
    "#% What is the performance of your classifier for each of the languages Finnish, Japanese and Russian?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
