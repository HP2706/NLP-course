{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from datasets import load_dataset\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "from typing import List, Union, Any\n",
    "from instructor import Instructor\n",
    "# various utils for distilling features \n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Awaitable\n",
    "import instructor\n",
    "import asyncio\n",
    "from typing import AsyncGenerator\n",
    "from tqdm.asyncio import tqdm as async_tqdm\n",
    "\n",
    "import nest_asyncio\n",
    "from openai import AsyncOpenAI\n",
    "current_dir = os.getcwd()\n",
    "if current_dir.endswith(\"week1\"):\n",
    "    os.chdir(\"..\")\n",
    "else:\n",
    "    print(\"current dir\", current_dir)\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "ds_val = ds[\"validation\"].to_pandas()\n",
    "ds_train = ds[\"train\"].to_pandas()\n",
    "ds_train = ds_train[ds_train['lang'].isin(['fi', 'ja', 'ru'])]\n",
    "ds_val = ds_val[ds_val['lang'].isin(['fi', 'ja', 'ru'])]\n",
    "print(len(ds_train))\n",
    "print(len(ds_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute embeddings for all\n",
    "async def embed_chunk(chunk: List[str]) -> Awaitable[List[float]]:\n",
    "    client = AsyncOpenAI()\n",
    "    \n",
    "    # Filter out empty strings and None values\n",
    "    filtered_chunk = [text for text in chunk if text and isinstance(text, str)]\n",
    "    \n",
    "    if not filtered_chunk:\n",
    "        print(\"Warning: Empty chunk after filtering\")\n",
    "        return [None] * len(chunk)  # Return None for each original item\n",
    "    \n",
    "    try:\n",
    "        response = await client.embeddings.create(input=filtered_chunk, model='text-embedding-ada-002')\n",
    "        embeddings = [r.embedding for r in response.data]\n",
    "        \n",
    "        # Pad the result with None for any filtered out items\n",
    "        result = []\n",
    "        filtered_index = 0\n",
    "        for item in chunk:\n",
    "            if item and isinstance(item, str):\n",
    "                result.append(embeddings[filtered_index])\n",
    "                filtered_index += 1\n",
    "            else:\n",
    "                result.append(None)\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding chunk: {e}\")\n",
    "        raise ValueError(f\"Got exception: {e} on chunk: {filtered_chunk}\")\n",
    "\n",
    "# ... rest of the code remains the same ...\n",
    "\n",
    "import itertools\n",
    "import asyncio\n",
    "from typing import Optional\n",
    "from tqdm.asyncio import tqdm as async_tqdm\n",
    "\n",
    "async def embed_all(df: pd.DataFrame):\n",
    "    cols = ['question', 'context', 'answer_inlang', 'answer']\n",
    "    batch_size = 128\n",
    "\n",
    "    async def process_batch(task_id: int, batch: List[Optional[str]]) -> tuple[int, List[Optional[List[float]]]]:\n",
    "        None_indices = [i for i, text in enumerate(batch) if text is None]\n",
    "        batch = [text for text in batch if text is not None]\n",
    "        try:\n",
    "            embeddings = await embed_chunk(batch)\n",
    "        except Exception as e:\n",
    "            print(f\"Error embedding batch: {e}\")\n",
    "            raise e\n",
    "        \n",
    "        for i in None_indices:\n",
    "            embeddings.insert(i, None)\n",
    "        return task_id, embeddings\n",
    "\n",
    "    all_texts = []\n",
    "    for col in cols:\n",
    "        all_texts.extend(df[col].tolist())\n",
    "\n",
    "    batches = [all_texts[i:i+batch_size] for i in range(0, len(all_texts), batch_size)]\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    tasks = [process_batch(task_id=i, batch=batch) for i, batch in enumerate(batches)]\n",
    "\n",
    "    async for embedding in async_tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Embedding\"):\n",
    "        all_embeddings.append(await embedding)\n",
    "\n",
    "    # Sort embeddings by task_id to maintain order\n",
    "    all_embeddings.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Flatten the embeddings list\n",
    "    flattened_embeddings = [emb for _, batch_emb in all_embeddings for emb in batch_emb]\n",
    "\n",
    "    # Assign embeddings to the correct columns in the DataFrame\n",
    "    for i, col in enumerate(cols):\n",
    "        start_idx = i * len(df)\n",
    "        end_idx = (i + 1) * len(df)\n",
    "        df[f'{col}_embedding'] = flattened_embeddings[start_idx:end_idx]\n",
    "\n",
    "    return df\n",
    "\n",
    "def embed_and_save(ds_train, ds_val):\n",
    "    for name, df in [(\"train\", ds_train)]:# (\"val\", ds_val)]:\n",
    "        df_with_embeddings = asyncio.run(embed_all(df))    \n",
    "        path = \"nlp_course/dataset\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        df_with_embeddings.to_parquet(f\"{path}/{name}_df.parquet\", index=False)\n",
    "        \n",
    "embed_and_save(ds_train, ds_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfemb = pd.read_parquet(\"nlp_course/dataset/train_df.parquet\")\n",
    "\n",
    "for col in ['question', 'context', 'answer']:\n",
    "    print(f\"{col} has {dfemb[f'{col}_embedding'].isnull().sum()} missing values\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [(_dict[\"lang\"], _dict[\"question\"]) for _dict in ds_train[['lang', 'question']].to_dict(orient='records')]\n",
    "print(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" import os\n",
    "import pandas as pd\n",
    "from pandasai import Agent \"\"\"\n",
    "\n",
    "print(ds_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize \n",
    "#we load the llama3 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_column(\n",
    "    df : pd.DataFrame, \n",
    "    tokenizer : AutoTokenizer,\n",
    "    tokenizer_name : str\n",
    "):\n",
    "    text_cols = ['question', 'context', 'answer']\n",
    "    for col in tqdm(text_cols):\n",
    "        df[f\"{col}_{tokenizer_name}_n_tokens\"] = df[col].apply(lambda x: len(tokenizer.encode(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize basic data statistics for train-\n",
    "#ing and validation data in each of the languages Finnish (fi), Japanese\n",
    "#(ja) and Russian (ru).\n",
    "\n",
    "# distribution of answerable and not answerable questions\n",
    "def create_language_plots(train_data, val_data, title_prefix):\n",
    "    # Create subplots\n",
    "    fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]],\n",
    "                        subplot_titles=(f\"{title_prefix} - Training Data\", \n",
    "                                        f\"{title_prefix} - Validation Data\"))\n",
    "\n",
    "    # Only fi, ja, ru\n",
    "    selected_langs = ['fi', 'ja', 'ru']\n",
    "    dfs = [train_data, val_data]\n",
    "    \n",
    "    figs = []\n",
    "    for df in dfs:\n",
    "        selected_data = df[df['lang'].isin(selected_langs)]\n",
    "        fig = px.sunburst(selected_data, path=['lang', 'answerable'],\n",
    "                       color='lang',\n",
    "                       color_discrete_map={True: 'rgba(0,100,0,0.7)', False: 'rgba(220,20,60,0.7)'})\n",
    "        fig.update_traces(textinfo='label+percent parent')\n",
    "        figs.append(fig)\n",
    "    \n",
    "    for fig in figs:\n",
    "        fig.add_trace(fig.data[0], row=1, col=1)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(title_text=f\"{title_prefix} Language Distribution\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def create_language_histogram(train_data, title_prefix):\n",
    "    selected_langs = ['fi', 'ja', 'ru']\n",
    "    df = train_data[train_data['lang'].isin(selected_langs)]\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=selected_langs)\n",
    "    \n",
    "    for i, lang in enumerate(selected_langs, 1):\n",
    "        lang_data = df[df['lang'] == lang]\n",
    "        \n",
    "        # Answerable questions (True)\n",
    "        answerable_data = lang_data[lang_data['answerable']]['question_n_tokens']\n",
    "        # Non-answerable questions (False)\n",
    "        non_answerable_data = lang_data[~lang_data['answerable']]['question_n_tokens']\n",
    "        \n",
    "        # Calculate bin edges for both distributions\n",
    "        bins = np.histogram_bin_edges(np.concatenate([answerable_data, non_answerable_data]), bins=50)\n",
    "        \n",
    "        # Compute histograms\n",
    "        answerable_hist, _ = np.histogram(answerable_data, bins=bins)\n",
    "        non_answerable_hist, _ = np.histogram(non_answerable_data, bins=bins)\n",
    "        \n",
    "        # Normalize histograms\n",
    "        answerable_hist = answerable_hist / answerable_hist.sum()\n",
    "        non_answerable_hist = non_answerable_hist / non_answerable_hist.sum()\n",
    "        \n",
    "        # Plot normalized histograms\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=bins[:-1], y=answerable_hist,\n",
    "                   name=f'{lang} - Answerable',\n",
    "                   marker_color='blue',\n",
    "                   opacity=0.7),\n",
    "            row=1, col=i\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=bins[:-1], y=non_answerable_hist,\n",
    "                   name=f'{lang} - Non-answerable',\n",
    "                   marker_color='red',\n",
    "                   opacity=0.7),\n",
    "            row=1, col=i\n",
    "        )\n",
    "        \n",
    "        # Update x-axis and y-axis labels\n",
    "        fig.update_xaxes(title_text=\"Number of Tokens\", row=1, col=i)\n",
    "        fig.update_yaxes(title_text=\"Relative Frequency\" if i == 1 else None, row=1, col=i)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=f\"{title_prefix} - Distribution of Question Token Counts by Language and Answerability\",\n",
    "        barmode='overlay',\n",
    "        height=500,\n",
    "        width=1200\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "# Create and display the plot\n",
    "fig_train = create_language_histogram(ds_train, \"Training Data\")\n",
    "fig_train.write_image(\"plots/week1_a_lang_token_distribution_normalized.png\")\n",
    "\n",
    "# Create and display plots for training data\n",
    "\"\"\" fig_train = create_language_plots(ds_train, ds_val, \"Training Data\")\n",
    "fig_train.write_image(\"plots/week1_a_dataset.png\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% (b) For each of the languages Finnish, Japanese and Russian, report the 5 most common \n",
    "#% words in the questions from the training set. What kind of words are they?\n",
    "\n",
    "#collect all tokens from the questions, and count them\n",
    "import MeCab\n",
    "\n",
    "def get_top_words(df: pd.DataFrame, lang: str, n=5):\n",
    "    df_lang = df[df['lang'] == lang].copy()\n",
    "    \n",
    "    if lang == 'ja':\n",
    "        mecab = MeCab.Tagger(\"-Owakati\")  # Initialize MeCab tokenizer\n",
    "        df_lang.loc[:, 'words_question_tokens'] = df_lang['question'].apply(lambda x: mecab.parse(x).split())\n",
    "    else:\n",
    "        df_lang.loc[:, 'words_question_tokens'] = df_lang['question'].apply(lambda x: x.split(' '))\n",
    "    \n",
    "    all_tokens = np.concatenate(df_lang['words_question_tokens'].values)\n",
    "    unique, counts = np.unique(all_tokens, return_counts=True)\n",
    "    sorted_indices = np.argsort(counts)[::-1]\n",
    "    top_unique_tokens = unique[sorted_indices][:n]\n",
    "    top_tokens_dict = {token: int(count) for token, count in zip(top_unique_tokens, counts[sorted_indices][:n])}\n",
    "    return top_tokens_dict\n",
    "\n",
    "def visualize_top_tokens(\n",
    "    df : pd.DataFrame, \n",
    "    lang : str, \n",
    "    n=5\n",
    "):\n",
    "    top_tokens = get_top_words(df, lang, n)\n",
    "    fig = px.bar(x=list(top_tokens.keys()), y=list(top_tokens.values()), labels={'x':'Tokens', 'y':'Counts'})\n",
    "    fig.update_layout(title=f\"Top {n} Tokens in {lang}\")\n",
    "    fig.show()\n",
    "    return fig, top_tokens\n",
    "\n",
    "for lang in ['fi', 'ja', 'ru']:\n",
    "    fig, top_tokens = visualize_top_tokens(ds_train, lang, 5)\n",
    "    fig.write_image(f\"plots/week1_b_top_5_tokens_{lang}.png\")\n",
    "    print(top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% (c) Implement a rule-based classifier that predicts whether a question is answerable \n",
    "#% or impossible, only using the document (context) and question. You may use machine \n",
    "#% translation as a component. Use the answerable field to evaluate it on the validation set. \n",
    "#% What is the performance of your classifier for each of the languages Finnish, Japanese and Russian?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
