
% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{import}

% Add this line to define \textrussian command
\newcommand{\textrussian}[1]{\foreignlanguage{russian}{#1}}

\lstset{
    language=Python,
    breaklines=true,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue},
    linewidth=0.5\textwidth,  % Set the width to 90% of the text width
}
\usepackage{graphicx}
\graphicspath{{../plots/}}


\begin{document}

\nolinenumbers  % Add this line to ensure line numbers are turned off


\title{NLP Course Report}
\author{Hans Peter Lyngsøe, pvr448}

\maketitle

\section{Week 1}
% (a) Explore the dataset from https://huggingface.co/datasets/coastalcph/tydi_xor_rc. 
% Familiarize yourself with the dataset card, download the dataset and explore its columns. 
% Summarize basic data statistics for training and validation data in each of the languages 
% Finnish (fi), Japanese (ja) and Russian (ru).
\begin{enumerate}
    \item[(a)] 

    Basic statistics:
    \begin{itemize}
        \item the data is quite evenly distributed across the 3 languages.
        \item We note that there are more answerable than unanswerable by a factor of 10-1.Which means we accuracy becomes a meaningless measure for performance.
        \item train\_set: 15326
        \item val\_set: 3028
    \end{itemize}

    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.3\textwidth]{week1_a_dataset.png}
        \caption{Distribution of labels in the dataset}
        \label{fig:label_distribution}
    \end{figure}

    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.5\textwidth]{week1_a_lang_token_distribution_normalized.png}
        \caption{normalized Histogram for token count(using llama3 tokenizer) of answerable/unanswerable questions in the dataset}
        \label{fig:language_distribution}
    \end{figure}

    % (b) For each of the languages Finnish, Japanese and Russian, report the 5 most common 
    % words in the questions from the training set. What kind of words are they?
    \item[(b)] 

    In order to get a faithful representation of the most meaningful words, we opt to filter out stopwords arguing that these words are not meaningful and does not carry information about the question.
    We use the NLP library spacy that provides and index of stopwords for each language. Additionally we remove common symbols like '.' and '?'.

    we use the fact that both finnish and russian uses spaces to seperate their words, and thus we can extract distinct words using the space as delimiter.
    Japanese is somewhat different as a language, and it is unclear whether naive splitting on space is meaningful. Thus we use a japanese speific tokenizer to parse sentences into words.
    we get the following results:

    \begin{figure}[t]
        \centering
        \begin{subfigure}[b]{0.1\textwidth}
            \centering
            \includegraphics[width=\textwidth]{week1_b_top_5_tokens_fi.png}
            \caption{Finnish}
            \label{fig:top_5_tokens_fi}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.1\textwidth}
            \centering
            \includegraphics[width=\textwidth]{week1_b_top_5_tokens_ja.png}
            \caption{Japanese}
            \label{fig:top_5_tokens_ja}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.1\textwidth}
            \centering
            \includegraphics[width=\textwidth]{week1_b_top_5_tokens_ru.png}
            \caption{Russian}
            \label{fig:top_5_tokens_ru}
        \end{subfigure}
        \caption{Top 5 tokens in Finnish, Japanese, and Russian}
        \label{fig:top_5_tokens_all}
    \end{figure}

    NOTE: todo use an embedding model to get a sense for the meaning of the words, consider how you do tokenization and 
    what the implications are. what are words and what are stopwords

    % (c) Implement a rule-based classifier that predicts whether a question is answerable 
    % or impossible, only using the document (context) and question. You may use machine 
    % translation as a component. Use the answerable field to evaluate it on the validation set. 
    % What is the performance of your classifier for each of the languages Finnish, Japanese and Russian?
    \item[(c)] 
    
    As a simple rule based approach explore the relationship between word/token count and answerability in the training set. 
    We expect that as a rough heuristic that longer questions are harder to answer. 
    In the appendix, we provide scatter plots for each language. See Figures~\ref{fig:scatter_week1_c_fi}, \ref{fig:scatter_week1_c_ja}, and \ref{fig:scatter_week1_c_ru} show the scatter plots for Finnish, Japanese, and Russian data respectively.
    based on this information, we decide to make a rule based classifier that predicts unanwerable for questions above a specific length that is different for each language.
    We select this length by looking at the training distribution. 
    ('ja': 10, 'ru': 15, 'fi': 9)

    we get the following results on the validation set:

    %% ja: {'accuracy': 0.5877192982456141, 'balanced_accuracy': 0.53745129167268, 'f1': 0.6907894736842105, 'precision': 0.6542056074766355, 'recall': 0.7317073170731707}
    %% ru: {'accuracy': 0.7121212121212122, 'balanced_accuracy': 0.4964788732394366, 'f1': 0.831858407079646, 'precision': 0.7157360406091371, 'recall': 0.9929577464788732}
    %% fi: {'accuracy': 0.6912878787878788, 'balanced_accuracy': 0.5132645803698435, 'f1': 0.81068524970964, 'precision': 0.7255717255717256, 'recall': 0.9184210526315789}
    %% all: {'accuracy': 0.6630434782608695, 'balanced_accuracy': 0.5284132271513975, 'f1': 0.783418723800652, 'precision': 0.7031772575250836, 'recall': 0.8843322818086226}
    \begin{table}[ht]
        \centering
        \begin{tabular}{|l|c|c|c|c|}
            \hline
            Language & Balanced Accuracy & F1 Score & Precision & Recall \\
            \hline
            All & 0.5284 & 0.7834 & 0.7032 & 0.8843 \\
            Finnish (fi) & 0.5133 & 0.8107 & 0.7256 & 0.9184 \\
            Japanese (ja) & 0.5375 & 0.6908 & 0.6542 & 0.7317 \\
            Russian (ru) & 0.4965 & 0.8319 & 0.7157 & 0.9930 \\
            \hline
        \end{tabular}
        \caption{Performance metrics of the rule-based classifier on the validation set}
        \label{tab:classifier_performance}
    \end{table}

    There are many downsides to this approach: 
    first of all although length might be correlated with answerability this is a vary noisy signal as length is a bad measure of complexity.
    furthermore we are putting a lot of faith in the tokenization step. And the predictions of our model could be meaningfully change with a different tokenization scheme.

    to measure if this is even better than sampling from a normal distribution with mean of the answerability in the training data. 
    %% ja: {'balanced_accuracy': 0.5069150361833289, 'f1': 0.7237738619087893, 'precision': 0.6332383607326422, 'recall': 0.8445993031358885}
    %% ru: {'balanced_accuracy': 0.5000628772635815, 'f1': 0.7947875573798628, 'precision': 0.7172864001600983, 'recall': 0.8911971830985917}
    %% fi: {'balanced_accuracy': 0.5058072546230441, 'f1': 0.7955247534892942, 'precision': 0.7223537184801281, 'recall': 0.8852631578947369}
    %% _all: {'balanced_accuracy': 0.5021206973888361, 'f1': 0.7682181356315498, 'precision': 0.6901554735311917, 'recall': 0.8662460567823344}
    \begin{table}[ht]
        \centering
        \begin{tabular}{|l|c|c|c|c|}
            \hline
            Language & Balanced Accuracy & F1 Score & Precision & Recall \\
            \hline
            All & 0.5021 & 0.7682 & 0.6902 & 0.8662 \\
            Finnish (fi) & 0.5058 & 0.7955 & 0.7224 & 0.8853 \\
            Japanese (ja) & 0.5069 & 0.7238 & 0.6332 & 0.8446 \\
            Russian (ru) & 0.5001 & 0.7948 & 0.7173 & 0.8912 \\
            \hline
        \end{tabular}
        \caption{Performance metrics of the random classifier on the validation set}
        \label{tab:random_classifier_performance}
    \end{table}

\end{enumerate}

\section{Week 37 (9--15 September)}
% Let k be the number of members in your group (k ∈ {1, 2, 3}). Implement k different 
% language models for the questions in the three languages Finnish, Japanese and Russian, 
% as well as for the document contexts in English (total k × 4 language models), using the 
% training data. Evaluate each of them on the validation data, report their performance 
% and discuss the results.

We train 3 different N-Gram models, one for each language.

Curiously enough we find that as we scale the number of tokens in the training data, the performance of the model gets worse.

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|r|r|}
        \hline
        Language & Training Tokens/Words & Perplexity \\
        \hline
        Japanese (ja) & 24,533 & 548.79 \\
        Russian (ru) & 16,526 & 140.53 \\
        Finnish (fi) & 13,013 & 91.90 \\
        Context (English) & 751,154 & 616.85 \\
        \hline
    \end{tabular}
    \caption{Language Model Performance}
    \label{tab:language_model_performance}
\end{table}



\section{Week 38 (16--22 September)}
% Let k be the number of members in your group. For each of the three languages Finnish, 
% Japanese and Russian separately, using the training data, train k different classifiers 
% that receive the document (context) and question as input and predict whether the question 
% is answerable or impossible given the context. Evaluate the classifiers on the respective 
% validation sets, report and analyse the performance for each language and compare the 
% scores across languages.

We train a simple linear layer mapping from an embedding space to the 1d classification space.We use openai's 'text-embedding-ada-002' model to embed the question and context and train on the cocatenation of the two 
This gives us a feature space of dimension 2*3072 = 6144.

We train a simple linear layer that takes the feature space as input and returns a binary probability distribution. we train using the binary crossentropy-loss.



\begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|l|c|c|c|c|c|c|}
    \hline
    Training & Evaluation & Context & D\_in & Expansion & Balanced & Precision & Recall \\
    & & & & Factor & Accuracy & & \\
    \hline
    Finnish & Finnish & No & 1536 & 2 & 0.7045 & 0.9664 & 1.0000 \\
    Finnish & Finnish & Yes & 3072 & 2 & 0.7955 & 0.9765 & 1.0000 \\
    Japanese & Japanese & No & 1536 & 2 & 0.7625 & 0.9105 & 0.9519 \\
    Japanese & Japanese & Yes & 3072 & 2 & 0.5244 & 0.8274 & 1.0000 \\
    Russian & Russian & No & 1536 & 2 & 0.9031 & 0.9825 & 0.9492 \\
    Russian & Russian & Yes & 3072 & 2 & 0.7058 & 0.9355 & 0.9831 \\
    \hline
    \end{tabular}%
    }
    \caption{Model performance for language-specific and combined training, with and without context}
    \label{tab:model_performance}
\end{table}

we also try to train a single model on all languages which meaningfully boosts performance 
but when context embedding. which suggests that there is a certain degree of transfer learning across the language tasks.

\begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|l|c|c|c|c|c|c|}
    \hline
    Training & Evaluation & Context & D\_in & Expansion & Balanced & Precision & Recall \\
    & & & & Factor & Accuracy & & \\
    \hline
    All & Finnish & No & 1536 & 2 & 0.9414 & 0.9933 & 0.9364 \\
    All & Finnish & Yes & 3072 & 2 & 0.9225 & 0.9888 & 0.9343 \\
    All & Japanese & No & 1536 & 2 & 0.6688 & 0.8741 & 0.9840 \\
    All & Japanese & Yes & 3072 & 2 & 0.8663 & 0.9559 & 0.9278 \\
    All & Russian & No & 1536 & 2 & 0.7955 & 0.9765 & 1.0000 \\
    All & Russian & Yes & 3072 & 2 & 0.8810 & 0.9867 & 0.9893 \\
    \hline
    \end{tabular}%
    }
    \caption{Model performance for language-specific and combined training, with and without context}
    \label{tab:model_performance}

\end{table}


\section{Week 39 (23--29 September)}
% We now move from binary classification to span-based QA, i.e. identifying the
% span in the document that answers the question.
% Let k be the number of members in your group. Using the training data in
% Finnish, Japanese and Russian separately, train k different sequence labellers,
% which predict the tokens in a document context that constitute the answer to
% the corresponding question.9 You can decide whether to train one model per
% language or a single model for all three languages. Evaluate using a sequence la-
% belling metric on the validation set, report and analyse the performance for each
% language and compare the scores across languages. Note that if the question is
% unanswerable, a correct output must be empty (contain no tokens).


we experiment with various sequence labeling models. we finetune, distilbert and deberta\_V3 and in both cases we get a perfect match of 50%

However we note that there is a meaningful discrepancy between how the model performs on finnish vs russian and japanese. As distilbert and deberta is not explicitly intended for multilingual tasks.
Thus we opt for using the mt5 base model that is a t5 encoder decoder model that is trained on both japanese, russian and finnish along with many other languages.
Instead of doing a whole finetune we freeze the weights and use a lora adapter 




\begin{itemize}
    \item 
    finetune for one language only and compare with the performance when training on all languages
    measure before and after finetuning perf on validation set.
\end{itemize}

we start by finetuning distillbert on all languages.


we use the same metrics as established in the squad v2 evaluation paper. \cite{rajpurkar-etal-2018-know}

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Epoch & Language & Exact Match & F1 Score & Own Exact Match & Own F1 \\
        \hline
        0 & Finnish (fi) & 16.10 & 17.27 & 0.1610 & 0.1710 \\
        0 & Japanese (ja) & 19.96 & 16.49 & 0.1996 & 0.1638 \\
        0 & Russian (ru) & 12.88 & 17.24 & 0.1288 & 0.1724 \\
        \hline
        2 & Finnish (fi) & 21.97 & 22.98 & 0.2197 & 0.2277 \\
        2 & Japanese (ja) & 26.32 & 17.64 & 0.2610 & 0.1753 \\
        2 & Russian (ru) & 19.95 & 22.00 & 0.1970 & 0.2186 \\
        \hline
        4 & Finnish (fi) & 27.27 & 27.39 & 0.2727 & 0.2710 \\
        4 & Japanese (ja) & 21.27 & 19.28 & 0.2127 & 0.1919 \\
        4 & Russian (ru) & 18.94 & 21.50 & 0.1894 & 0.2143 \\
        \hline
        6 & Finnish (fi) & 28.22 & 27.37 & 0.2803 & 0.2709 \\
        6 & Japanese (ja) & 22.59 & 17.74 & 0.2259 & 0.1772 \\
        6 & Russian (ru) & 18.94 & 21.27 & 0.1869 & 0.2105 \\
        \hline
        8 & Finnish (fi) & 27.27 & 26.68 & 0.2727 & 0.2646 \\
        8 & Japanese (ja) & 23.25 & 18.49 & 0.2325 & 0.1843 \\
        8 & Russian (ru) & 18.43 & 22.26 & 0.1818 & 0.2213 \\
        \hline
        10 & Finnish (fi) & 27.08 & 27.51 & 0.2689 & 0.2728 \\
        10 & Japanese (ja) & 21.93 & 19.07 & 0.2193 & 0.1901 \\
        10 & Russian (ru) & 16.92 & 21.07 & 0.1667 & 0.2094 \\
        \hline
    \end{tabular}
    \caption{Evaluation metrics for different languages at various epochs}
    \label{tab:evaluation_metrics}
\end{table}


from this data 
we note that bert and distillbert are not explicitly trained for multilingual tasks and thus it falls short. 
we note that the model seems to be consequently worse at 

deberta\_v3 

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        Language & Exact Match & F1 Score & Own Exact Match & Own F1 \\
        \hline
        Finnish (fi) & 0.7576 & 5.2205 & 0.0 & 0.0572 \\
        Japanese (ja) & 0.8772 & 4.7133 & 0.0022 & 0.0474 \\
        Russian (ru) & 0.0 & 5.4960 & 0.0 & 0.0541 \\
        \hline
        Finnish (fi) & 40.7197 & 45.4617 & 40.1515 & 45.0355 \\
        Japanese (ja) & 49.3421 & 42.6790 & 48.6842 & 42.1142 \\
        Russian (ru) & 41.9192 & 48.0977 & 41.9192 & 48.0750 \\
        \hline
        Finnish (fi) & 42.8030 & 45.6286 & 42.6136 & 45.1638 \\
        Japanese (ja) & 51.5351 & 44.4455 & 51.0965 & 43.9791 \\
        Russian (ru) & 41.1616 & 48.0781 & 41.1616 & 48.0541 \\
        \hline
        Finnish (fi) & 41.8561 & 46.1020 & 41.8561 & 45.7348 \\
        Japanese (ja) & 50.0000 & 44.3151 & 49.5614 & 43.8437 \\
        Russian (ru) & 41.9192 & 47.9060 & 41.9192 & 47.7982 \\
        \hline
        Finnish (fi) & 41.8561 & 46.1203 & 41.8561 & 45.7363 \\
        Japanese (ja) & 49.1228 & 44.8132 & 48.6842 & 44.3369 \\
        Russian (ru) & 42.1717 & 48.2566 & 42.1717 & 48.1539 \\
        \hline
    \end{tabular}
    \caption{Evaluation metrics for different languages}
    \label{tab:evaluation_metrics}
\end{table}

we try to finetune mt5 on the task which is a multilingual version of the t5 encoder-decoder model. 


\section{Week 40 (30 September--6 October)}
% Use the subset of the questions in Finnish, Japanese and Russian to train (or fine-tune) 
% an encoder-decoder model that receives the question and context as input and generates 
% the in-language answer. You can decide whether to train one model per language or a 
% single model for all three languages.

for this task we opt for finetuning a general pretrained large language model. Llama3-1-8B-instruct.



\begin{table}[ht]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        Language & \multicolumn{2}{c|}{Pre-finetuned} & \multicolumn{2}{c|}{Finetuned} \\
        \cline{2-5}
        & Exact Match & F1 Score & Exact Match & F1 Score \\
        \hline
        Finnish (fi) & 0.02 & 0.0837 & 0.11 & 0.1681 \\
        Japanese (ja) & 0.00 & 0.0040 & 0.15 & 0.1500 \\
        Russian (ru) & 0.00 & 0.0689 & 0.10 & 0.1612 \\
        \hline
        Overall & 0.0067 & 0.0522 & 0.12 & 0.1598 \\
        \hline
    \end{tabular}
    \caption{Performance comparison of pre-finetuned and finetuned llama3-1-8B-instruct models}
    \label{tab:llama3_performance_comparison}
\end{table}

\section{Week 41+ (from 7 October)}
% Use all questions in Finnish, Japanese and Russian to train (or fine-tune) an encoder-decoder 
% model that receives the question and context as input and generates the English answer. 
% You can decide whether to train one model per question language or a single model for all 
% three languages. Evaluate using a text generation metric on the validation set, and compare 
% the overall results between answerable and unanswerable examples.

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        Language & \multicolumn{2}{c|}{Pre-finetuned} & \multicolumn{2}{c|}{Finetuned} \\
        \cline{2-5}
        & Exact Match & F1 Score & Exact Match & F1 Score \\
        \hline
        Finnish (fi) & 0.0587 & 0.1498 & 0.4811 & 0.6095 \\
        Japanese (ja) & 0.0110 & 0.0752 & 0.5351 & 0.6310 \\
        Russian (ru) & 0.0480 & 0.1255 & 0.5025 & 0.6199 \\
        \hline
        Overall & 0.0399 & 0.1182 & 0.5051 & 0.6196 \\
        \hline
    \end{tabular}
    \caption{Performance comparison of pre-finetuned and finetuned models for English answer generation}
    \label{tab:week41_performance_comparison}
\end{table}


\section{Document Body}

\subsection{Footnotes}

Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

\subsection{Tables and figures}

See Table~\ref{tab:accents} for an example of a table and its caption.
\textbf{Do not override the default caption sizes.}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.25\textwidth]{week1_c_scatter_fi.png}
    \caption{Scatter plot for Finnish}
    \label{fig:scatter_week1_c_fi}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.25\textwidth]{week1_c_scatter_ja.png}
    \caption{Scatter plot for Japanese}
    \label{fig:scatter_week1_c_ja}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.25\textwidth]{week1_c_scatter_ru.png}
    \caption{Scatter plot for Russian}
    \label{fig:scatter_week1_c_ru}
\end{figure}


\subsection{Hyperlinks}

Users of older versions of \LaTeX{} may encounter the following error during compilation:
\begin{quote}
\verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
\end{quote}
This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

\subsection{Citations}

\begin{table*}
  \centering
  \begin{tabular}{lll}
    \hline
    \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
    \hline
    \citep{Gusfield:97}       & \verb|\citep|           &                           \\
    \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
    \citet{Gusfield:97}       & \verb|\citet|           &                           \\
    \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
    \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
    \hline
  \end{tabular}
  \caption{\label{citation-guide}
    Citation commands supported by the style file.
    The style is based on the natbib package and supports all natbib citation commands.
    It also supports commands defined in previous ACL style files for compatibility.
  }
\end{table*}

Table~\ref{citation-guide} shows the syntax supported by the style files.
We encourage you to use the natbib styles.
You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

A possessive citation can be made with the command \verb|\citeposs|.
This is not a standard natbib command, so it is generally not compatible
with other style files.

\subsection{References}

\nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
\begin{quote}
\begin{verbatim}
\bibliography{custom}
\end{verbatim}
\end{quote}

You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
To include both the Anthology and your own .bib file, use the following instead of the above.
\begin{quote}
\begin{verbatim}
\bibliography{anthology,custom}
\end{verbatim}
\end{quote}

Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

\subsection{Equations}

An example equation is shown below:
\begin{equation}
  \label{eq:example}
  A = \pi r^2
\end{equation}

Labels for equation numbers, sections, subsections, figures and tables
are all defined with the \verb|\label{label}| command and cross references
to them are made with the \verb|\ref{label}| command.

This an example cross-reference to Equation~\ref{eq:example}.

\subsection{Appendices}

\appendix
Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.




\section{Bib\TeX{} Files}
\label{sec:bibtex}


\end{document}