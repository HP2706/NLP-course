{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "from tqdm.asyncio import tqdm as async_tqdm\n",
    "import nltk\n",
    "from typing import Literal, Optional, Union\n",
    "from nltk import ngrams\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, padded_everygrams\n",
    "##imports\n",
    "import nest_asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from typing import Optional\n",
    "from tqdm.asyncio import tqdm as async_tqdm\n",
    "from spacy.lang.ja import Japanese\n",
    "from spacy.lang.ru import Russian\n",
    "from spacy.lang.fi import Finnish\n",
    "import nltk\n",
    "#download the knbc corpus\n",
    "nltk.download('knbc')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import knbc\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "if current_dir.endswith(\"code\"):\n",
    "    os.chdir(\"..\")\n",
    "else:\n",
    "    print(\"current dir\", current_dir)\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = pd.read_parquet(\"dataset/train_df.parquet\")\n",
    "ds_val = pd.read_parquet(\"dataset/val_df.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Lidstone, KneserNeyInterpolated  # Add this import at the top of your file\n",
    "from nltk.util import pad_sequence, ngrams, everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends, flatten\n",
    "class NGramTrainer:\n",
    "    model: Optional[MLE] = None\n",
    "    def __init__(\n",
    "        self, \n",
    "        ds_train: pd.DataFrame,\n",
    "        ds_val: pd.DataFrame,\n",
    "        n: int, \n",
    "        lang: Optional[Literal['ja', 'ru', 'fi']] = None, \n",
    "        on_context: bool = False\n",
    "    ):\n",
    "\n",
    "        if lang:\n",
    "            ds_train = ds_train[ds_train['lang'] == lang]['question_tokens'].tolist()\n",
    "            ds_val = ds_val[ds_val['lang'] == lang]['question_tokens'].tolist()\n",
    "        elif on_context:\n",
    "            ds_train = ds_train['context_tokens'].tolist()\n",
    "            ds_val = ds_val['context_tokens'].tolist()\n",
    "        else:\n",
    "            raise ValueError(\"lang must be provided if on_context is False\")\n",
    "        \n",
    "        self.n = n\n",
    "        self.lang = lang\n",
    "        self.on_context = on_context\n",
    "        self.model = None\n",
    "        self.flattened_dataset = list(flatten(ds_train))\n",
    "        self.flattened_val_dataset = list(flatten(ds_val))\n",
    "        \n",
    "    def fit(self, slice : Optional[int] = None):\n",
    "        train_data, padded_sents = padded_everygram_pipeline(self.n, self.flattened_dataset[:slice])\n",
    "        # Create and train the model\n",
    "        model = Lidstone(order=self.n, gamma=0.1)  # gamma is the smoothing parameter\n",
    "        model.fit(train_data, padded_sents)\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def evaluate(self):\n",
    "        '''evaluate on the validation set'''\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "        perplexity = self.model.perplexity(self.flattened_val_dataset)\n",
    "        return perplexity\n",
    "    \n",
    "    def predict(self, text: str):\n",
    "        '''predict the next word'''\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "        return self.model.generate(text_seed=text)\n",
    "    \n",
    "    \"\"\" def save_model(self, name: str):\n",
    "        path = f\"models/ngram_{self.n}_ntokens_{len(self.flattened_dataset)}.pkl\"\n",
    "        self.model.save(path) \"\"\"\n",
    "    \n",
    "for lang in [100, 1000, 10000]:\n",
    "    Trainer = NGramTrainer(ds_train, ds_val, 3, lang=\"ru\")\n",
    "    Trainer.fit(lang)\n",
    "    perplexity = Trainer.evaluate()\n",
    "    print(f\"Perplexity for {lang}: {perplexity}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring scaling laws of ngrams\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def scaling_law(\n",
    "    n: int, \n",
    "    type: Literal['ja', 'ru', 'fi', 'context'],\n",
    "):\n",
    "    if type == 'context':\n",
    "        Trainer = NGramTrainer(ds_train, ds_val, n, on_context=True)\n",
    "    else:\n",
    "        Trainer = NGramTrainer(ds_train, ds_val, n, on_context=False, lang=type)\n",
    "    \n",
    "    size = len(Trainer.flattened_dataset)\n",
    "    \n",
    "    # Create logarithmically spaced points\n",
    "    n_points = 20  # Increase number of points for smoother curve\n",
    "    slices = np.logspace(2, np.log10(size), num=n_points, dtype=int)\n",
    "    print(\"slices\", slices)\n",
    "    perplexity_list = []\n",
    "    for slice in tqdm(slices):\n",
    "        Trainer.fit(slice)\n",
    "        perplexity = Trainer.evaluate()\n",
    "        perplexity_list.append({'slice': slice, 'perplexity': perplexity})\n",
    "    print(\"perplexity_list\", perplexity_list)\n",
    "    df = pd.DataFrame(perplexity_list)\n",
    "    fig = px.line(\n",
    "        df, \n",
    "        x='slice', \n",
    "        y='perplexity', \n",
    "        title=f\"Perplexity Scaling Law for {type} with n={n}\", \n",
    "        labels={\"slice\": \"Number of tokens\", \"perplexity\": \"Perplexity\"},\n",
    "        log_x=True,  # Use log scale for x-axis\n",
    "    )\n",
    "    fig.update_xaxes(type=\"log\")\n",
    "    return fig\n",
    "\n",
    "# Run the analysis for different n-gram sizes and languages\n",
    "#for n in [1, 2, 3]:\n",
    "    #for lang in ['ja', 'ru', 'fi']:\n",
    "    #    scaling_law(n, lang).show()\n",
    "\n",
    "# Also run for context if needed\n",
    "#scaling_law(2, 'context').show()\n",
    "\n",
    "scaling_law(3, 'ru').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logarithmically spaced points\n",
    "n_points = 20  # Increase number of points for smoother curve\n",
    "slices = np.logspace(2, np.log10(10000), num=n_points, dtype=int)\n",
    "print(\"slices\", slices)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
