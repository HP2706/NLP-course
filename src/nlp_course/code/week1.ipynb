{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from datasets import load_dataset\n",
    "from plotly.subplots import make_subplots\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "from typing import List\n",
    "from typing import Awaitable\n",
    "from tqdm.asyncio import tqdm as async_tqdm\n",
    "import nest_asyncio\n",
    "from openai import AsyncOpenAI\n",
    "import MeCab\n",
    "import numpy as np\n",
    "import itertools\n",
    "import asyncio\n",
    "from typing import Optional\n",
    "from tqdm.asyncio import tqdm as async_tqdm\n",
    "from spacy.lang.ja import Japanese\n",
    "from spacy.lang.ru import Russian\n",
    "from spacy.lang.fi import Finnish\n",
    "import nltk\n",
    "#download the knbc corpus\n",
    "nltk.download('knbc')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import knbc\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "if current_dir.endswith(\"code\"):\n",
    "    os.chdir(\"..\")\n",
    "else:\n",
    "    print(\"current dir\", current_dir)\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Preprocessing\n",
    "##Load the dataset, embeddings and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"dataset/train_df.parquet\"):\n",
    "    ds_train = pd.read_parquet(\"dataset/train_df.parquet\")\n",
    "    ds_val = pd.read_parquet(\"dataset/val_df.parquet\")\n",
    "else:\n",
    "    print(\"Dataset should be built, run embed_and_save first\")\n",
    "    #ds_train, ds_val = embed_and_save(ds_train, ds_val)\n",
    "    ds = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "    ds_val = ds[\"validation\"].to_pandas()\n",
    "    ds_train = ds[\"train\"].to_pandas()\n",
    "    ds_train = ds_train[ds_train['lang'].isin(['fi', 'ja', 'ru'])]\n",
    "    ds_val = ds_val[ds_val['lang'].isin(['fi', 'ja', 'ru'])]\n",
    "    print(len(ds_train))\n",
    "    print(len(ds_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute embeddings for all\n",
    "async def embed_chunk(chunk: List[str]) -> Awaitable[List[float]]:\n",
    "    client = AsyncOpenAI()\n",
    "    \n",
    "    # Filter out empty strings and None values\n",
    "    filtered_chunk = [text for text in chunk if text and isinstance(text, str)]\n",
    "    \n",
    "    if not filtered_chunk:\n",
    "        print(\"Warning: Empty chunk after filtering\")\n",
    "        return [None] * len(chunk)  # Return None for each original item\n",
    "    \n",
    "    try:\n",
    "        response = await client.embeddings.create(input=filtered_chunk, model='text-embedding-ada-002')\n",
    "        embeddings = [r.embedding for r in response.data]\n",
    "        \n",
    "        # Pad the result with None for any filtered out items\n",
    "        result = []\n",
    "        filtered_index = 0\n",
    "        for item in chunk:\n",
    "            if item and isinstance(item, str):\n",
    "                result.append(embeddings[filtered_index])\n",
    "                filtered_index += 1\n",
    "            else:\n",
    "                result.append(None)\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding chunk: {e}\")\n",
    "        raise ValueError(f\"Got exception: {e} on chunk: {filtered_chunk}\")\n",
    "\n",
    "async def embed_all(df: pd.DataFrame):\n",
    "    cols = ['question', 'context', 'answer_inlang', 'answer']\n",
    "    batch_size = 128\n",
    "\n",
    "    async def process_batch(task_id: int, batch: List[Optional[str]]) -> tuple[int, List[Optional[List[float]]]]:\n",
    "        None_indices = [i for i, text in enumerate(batch) if text is None]\n",
    "        batch = [text for text in batch if text is not None]\n",
    "        try:\n",
    "            embeddings = await embed_chunk(batch)\n",
    "        except Exception as e:\n",
    "            print(f\"Error embedding batch: {e}\")\n",
    "            raise e\n",
    "        \n",
    "        for i in None_indices:\n",
    "            embeddings.insert(i, None)\n",
    "        return task_id, embeddings\n",
    "\n",
    "    all_texts = []\n",
    "    for col in cols:\n",
    "        all_texts.extend(df[col].tolist())\n",
    "\n",
    "    batches = [all_texts[i:i+batch_size] for i in range(0, len(all_texts), batch_size)]\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    tasks = [process_batch(task_id=i, batch=batch) for i, batch in enumerate(batches)]\n",
    "\n",
    "    async for embedding in async_tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Embedding\"):\n",
    "        all_embeddings.append(await embedding)\n",
    "\n",
    "    # Sort embeddings by task_id to maintain order\n",
    "    all_embeddings.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Flatten the embeddings list\n",
    "    flattened_embeddings = [emb for _, batch_emb in all_embeddings for emb in batch_emb]\n",
    "\n",
    "    # Assign embeddings to the correct columns in the DataFrame\n",
    "    for i, col in enumerate(cols):\n",
    "        start_idx = i * len(df)\n",
    "        end_idx = (i + 1) * len(df)\n",
    "        df[f'{col}_embedding'] = flattened_embeddings[start_idx:end_idx]\n",
    "\n",
    "    return df\n",
    "\n",
    "def embed_and_save(ds_train, ds_val):\n",
    "    for name, df in [(\"train\", ds_train), (\"val\", ds_val)]:\n",
    "        df_with_embeddings = asyncio.run(embed_all(df))    \n",
    "        path = \"dataset\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        df_with_embeddings.to_parquet(f\"{path}/{name}_df.parquet\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize \n",
    "\n",
    "# a list of manually set stopwords\n",
    "other_stopwords = ['.', '?', '!', ',', ':', ';', '(', ')', '[', ']', '{', '}', '|', '\\\\', '/', '*', '+', '-', '=', '_', '^', '~', '<', '>', '\\\"', '\\'', '…', '“', '”', '–', '—', '...', '..', '...']\n",
    "#add spacing to stopwords\n",
    "left_over_stopwords = [word + \" \" for word in other_stopwords]\n",
    "right_over_stopwords = [\" \" + word for word in other_stopwords]\n",
    "numbers_stopwords = [str(i) for i in range(2024)] \n",
    "\n",
    "\n",
    "all_stopwords = left_over_stopwords + right_over_stopwords + numbers_stopwords\n",
    "\n",
    "nltk_finnish_stopwords = list(set(stopwords.words('finnish')))\n",
    "nltk_japanese_stopwords = list(set(knbc.words()))\n",
    "nltk_russian_stopwords = list(set(stopwords.words('russian')))\n",
    "#get stop words from spacy\n",
    "spacy_ja_stopwords = list(set(Japanese.Defaults.stop_words)) + all_stopwords \n",
    "spacy_ru_stopwords = list(set(Russian.Defaults.stop_words)) + all_stopwords\n",
    "spacy_fi_stopwords = list(set(Finnish.Defaults.stop_words)) + all_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE not currently used\n",
    "def tokenize_column(\n",
    "    df : pd.DataFrame, \n",
    "    tokenizer : AutoTokenizer,\n",
    "    tokenizer_name : str\n",
    "):\n",
    "    text_cols = ['question', 'context', 'answer']\n",
    "    for col in tqdm(text_cols):\n",
    "        df[f\"{col}_{tokenizer_name}\"] = df[col].apply(lambda x: tokenizer.encode(x))\n",
    "        df[f\"{col}_{tokenizer_name}_n_tokens\"] = df[f\"{col}_{tokenizer_name}\"].apply(lambda x: len(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "def spacy_tokenize_column(\n",
    "    df: pd.DataFrame, \n",
    "    lang: str\n",
    "):\n",
    "    text_cols = ['question', 'context', 'answer']\n",
    "    for col in tqdm(text_cols):\n",
    "        #NOTE only the question is in the language, ['context', 'answer'] are in english\n",
    "        if col == 'question':\n",
    "            df[f\"{col}_spacy{lang}\"] = df[col].apply(lambda x: tokenizer.encode(x))\n",
    "            df[f\"{col}_spacy{lang}_n_tokens\"] = df[f\"{col}_spacy{lang}\"].apply(lambda x: len(x))\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "#ds_train = tokenize_column(ds_train, tokenizer, \"meta-llama/Meta-Llama-3.1-8B\")\n",
    "#ds_val = tokenize_column(ds_val, tokenizer, \"meta-llama/Meta-Llama-3.1-8B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize basic data statistics for train-\n",
    "#ing and validation data in each of the languages Finnish (fi), Japanese\n",
    "#(ja) and Russian (ru).\n",
    "\n",
    "# distribution of answerable and not answerable questions\n",
    "def create_language_plots(train_data, val_data, title_prefix):\n",
    "    # Create subplots\n",
    "    fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]],\n",
    "                        subplot_titles=(f\"{title_prefix} - Training Data\", \n",
    "                                        f\"{title_prefix} - Validation Data\"))\n",
    "\n",
    "    # Only fi, ja, ru\n",
    "    selected_langs = ['fi', 'ja', 'ru']\n",
    "    dfs = [train_data, val_data]\n",
    "    \n",
    "    figs = []\n",
    "    for df in dfs:\n",
    "        selected_data = df[df['lang'].isin(selected_langs)]\n",
    "        fig = px.sunburst(selected_data, path=['lang', 'answerable'],\n",
    "                       color='lang',\n",
    "                       color_discrete_map={True: 'rgba(0,100,0,0.7)', False: 'rgba(220,20,60,0.7)'})\n",
    "        fig.update_traces(textinfo='label+percent parent')\n",
    "        figs.append(fig)\n",
    "    \n",
    "    for fig in figs:\n",
    "        fig.add_trace(fig.data[0], row=1, col=1)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(title_text=f\"{title_prefix} Language Distribution\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_language_histogram(train_data, title_prefix):\n",
    "    selected_langs = ['fi', 'ja', 'ru']\n",
    "    df = train_data[train_data['lang'].isin(selected_langs)]\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=selected_langs)\n",
    "    \n",
    "    for i, lang in enumerate(selected_langs, 1):\n",
    "        lang_data = df[df['lang'] == lang]\n",
    "        \n",
    "        # Answerable questions (True)\n",
    "        answerable_data = lang_data[lang_data['answerable']]['question_meta-llama/Meta-Llama-3.1-8B_n_tokens']\n",
    "        # Non-answerable questions (False)\n",
    "        non_answerable_data = lang_data[~lang_data['answerable']]['question_meta-llama/Meta-Llama-3.1-8B_n_tokens']\n",
    "        \n",
    "        # Calculate bin edges for both distributions\n",
    "        bins = np.histogram_bin_edges(np.concatenate([answerable_data, non_answerable_data]), bins=50)\n",
    "        \n",
    "        # Compute histograms\n",
    "        answerable_hist, _ = np.histogram(answerable_data, bins=bins)\n",
    "        non_answerable_hist, _ = np.histogram(non_answerable_data, bins=bins)\n",
    "        \n",
    "        # Normalize histograms\n",
    "        answerable_hist = answerable_hist / answerable_hist.sum()\n",
    "        non_answerable_hist = non_answerable_hist / non_answerable_hist.sum()\n",
    "        \n",
    "        # Plot normalized histograms\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=bins[:-1], y=answerable_hist,\n",
    "                   name=f'{lang} - Answerable',\n",
    "                   marker_color='blue',\n",
    "                   opacity=0.7),\n",
    "            row=1, col=i\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=bins[:-1], y=non_answerable_hist,\n",
    "                   name=f'{lang} - Non-answerable',\n",
    "                   marker_color='red',\n",
    "                   opacity=0.7),\n",
    "            row=1, col=i\n",
    "        )\n",
    "        \n",
    "        # Update x-axis and y-axis labels\n",
    "        fig.update_xaxes(title_text=\"Number of Tokens\", row=1, col=i)\n",
    "        fig.update_yaxes(title_text=\"Relative Frequency\" if i == 1 else None, row=1, col=i)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=f\"{title_prefix} - Distribution of Question Token Counts by Language and Answerability\",\n",
    "        barmode='overlay',\n",
    "        height=500,\n",
    "        width=1200\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "import instructor\n",
    "\n",
    "class TranslationObject(BaseModel):\n",
    "    '''i pydantic model for translating'''\n",
    "    original_text: str = Field(description=\"The text to be translated\")\n",
    "    translated_text: str = Field(description=\"The translation of the original text\")\n",
    "\n",
    "async def translate_text(\n",
    "    texts: List[str], \n",
    "    lang: str\n",
    ") -> List[str]:\n",
    "    client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "    texts_str = '\\n'.join(texts)\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", #dont change this line\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": f\"You are a helpful assistant that translates text from English to {lang}.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"\"\"\n",
    "                Translate the following texts from {lang} into English:\n",
    "                {texts_str}\n",
    "                \"\"\"\n",
    "            }\n",
    "        ],\n",
    "        response_model=List[TranslationObject]\n",
    "    )\n",
    "    return [f\"{translation.original_text} : {translation.translated_text}\" for translation in response]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% (b) For each of the languages Finnish, Japanese and Russian, report the 5 most common \n",
    "#% words in the questions from the training set. What kind of words are they?\n",
    "\n",
    "def get_top_words(\n",
    "    df: pd.DataFrame, \n",
    "    lang: str, \n",
    "    stopwords: List[str],\n",
    "    n=5\n",
    "):\n",
    "    df_lang = df[df['lang'] == lang].copy()\n",
    "    \n",
    "    lang_map = {\n",
    "        'ja': 'japanese',\n",
    "        'ru': 'russian',\n",
    "        'fi': 'finnish'\n",
    "    }\n",
    "    \n",
    "    if lang == 'ja':\n",
    "        japanese_tokenizer = Japanese()\n",
    "        df_lang.loc[:, 'words_question_tokens'] = df_lang['question'].apply(lambda x: [token.text for token in japanese_tokenizer(x)])\n",
    "    else:\n",
    "        df_lang.loc[:, 'words_question_tokens'] = df_lang['question'].apply(lambda x: word_tokenize(x, language=lang_map[lang]))\n",
    "    \n",
    "    \n",
    "    all_tokens = np.concatenate(df_lang['words_question_tokens'].values)\n",
    "    print(\"len before filtering\", len(all_tokens))\n",
    "    filtered_tokens = [word.lower() for word in all_tokens if word.isalnum() and word.lower() not in stopwords]\n",
    "    print(\"len after filtering\", len(filtered_tokens))\n",
    "    \n",
    "    unique, counts = np.unique(filtered_tokens, return_counts=True)\n",
    "    sorted_indices = np.argsort(counts)[::-1]\n",
    "    top_unique_tokens = unique[sorted_indices][:n]\n",
    "    top_tokens_dict = {token: int(count) for token, count in zip(top_unique_tokens, counts[sorted_indices][:n])}\n",
    "    return top_tokens_dict\n",
    "\n",
    "\n",
    "\n",
    "async def visualize_top_tokens(\n",
    "    df : pd.DataFrame, \n",
    "    lang : str, \n",
    "    stopwords : List[str],\n",
    "    n=5\n",
    "):\n",
    "    top_tokens = get_top_words(df, lang, stopwords, n)\n",
    "    \n",
    "    \n",
    "    top_tokens_list = list(top_tokens.keys())\n",
    "    top_tokens_with_translation = await translate_text(top_tokens_list, lang)\n",
    "    \n",
    "    fig = px.bar(x=list(top_tokens_with_translation), y=list(top_tokens.values()), labels={'x':'Tokens', 'y':'Counts'})\n",
    "    fig.update_layout(title=f\"Top {n} Tokens in {lang}\")\n",
    "    fig.show()\n",
    "    return fig, top_tokens\n",
    "\n",
    "for stopwords, lang in [\n",
    "    (spacy_ja_stopwords, 'ja'),\n",
    "    (spacy_ru_stopwords, 'ru'),\n",
    "    (spacy_fi_stopwords, 'fi')\n",
    "]:\n",
    "    fig, top_tokens = await visualize_top_tokens(ds_train, lang, stopwords, 5)\n",
    "    for key in top_tokens.keys():\n",
    "        key_in = key in stopwords\n",
    "        print(f\"{key} in stopwords: {key_in}\")\n",
    "\n",
    "    path = f\"plots/week1_b_top_5_tokens_{lang}.png\"\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    fig.write_image(path)\n",
    "    print(top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% (c) Implement a rule-based classifier that predicts whether a question is answerable \n",
    "#% or impossible, only using the document (context) and question. You may use machine \n",
    "#% translation as a component. Use the answerable field to evaluate it on the validation set. \n",
    "#% What is the performance of your classifier for each of the languages Finnish, Japanese and Russian?\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class RuleBasedClassifier(ABC):\n",
    "    @abstractmethod\n",
    "    def classify(self, question: str) -> bool:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def evaluate(self) -> float:\n",
    "        pass\n",
    "\n",
    "class SemanticSimilarityClassifier(RuleBasedClassifier):\n",
    "    def __init__(\n",
    "        self, \n",
    "        train_df: pd.DataFrame, \n",
    "        val_df: pd.DataFrame,\n",
    "        lang: str,\n",
    "        dims: int = 1536\n",
    "    ):\n",
    "        '''\n",
    "        train_df: pd.DataFrame  the training data\n",
    "        val_df: pd.DataFrame the validation data\n",
    "        dims: int  the number of dimensions we use for the semantic space \n",
    "        note that openai uses a matryoshka embedding model so we can \n",
    "        slice the embedding vector and still get a good representation\n",
    "        '''\n",
    "        self.dims = dims\n",
    "        self.lang = lang\n",
    "        self.train_df = train_df[train_df['lang'] == lang]\n",
    "        self.val_df = val_df[val_df['lang'] == lang]\n",
    "        \n",
    "    def collect_data(self):\n",
    "        question_embedding = np.stack(self.train_df['question_embedding'].values)\n",
    "        context_embedding = np.stack(self.train_df['context_embedding'].values)\n",
    "        answerables = np.stack(self.train_df['answerable'].values)\n",
    "        print(question_embedding.shape)\n",
    "        print(context_embedding.shape)\n",
    "        distances = np.linalg.norm(question_embedding - context_embedding, axis=1)\n",
    "        \n",
    "        return answerables, distances\n",
    "        \n",
    "        \n",
    "    def classify(\n",
    "        self, \n",
    "        question: List[str], \n",
    "        is_train: bool = False\n",
    "    ) -> tuple[bool, float]:\n",
    "        '''find the question embedding from the question'''\n",
    "        if is_train:\n",
    "            row = self.train_df[self.train_df['question'].isin(question)]\n",
    "        else:\n",
    "            row = self.val_df[self.val_df['question'].isin(question)]\n",
    "            \n",
    "        question_embedding = row['question_embedding'].values[0][:self.dims]\n",
    "        context_embedding = row['context_embedding'].values[0][:self.dims]\n",
    "        \n",
    "        print(type(question_embedding))\n",
    "        print(type(context_embedding))\n",
    "        #we measure euclidean normalized distance between question and context\n",
    "        dist = np.linalg.norm(question_embedding - context_embedding)\n",
    "        return bool(dist < 0.5), dist \n",
    "            \n",
    "    def evaluate(self, question: str):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from typing import Literal\n",
    "\n",
    "class TokenCountClassifier(RuleBasedClassifier):\n",
    "    def __init__(\n",
    "        self, \n",
    "        train_df: pd.DataFrame, \n",
    "        val_df: pd.DataFrame, \n",
    "    ):\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.lang_map = {\n",
    "            'ja': 'japanese',\n",
    "            'ru': 'russian',\n",
    "            'fi': 'finnish'\n",
    "        }\n",
    "        self.japanese_tokenizer = MeCab.Tagger(\"-Owakati\")\n",
    "        \n",
    "    def prepare_data(\n",
    "        self, \n",
    "        questions: List[str], \n",
    "        is_train: bool = True,\n",
    "        lang: Literal['ja', 'ru', 'fi', '_all'] = '_all',\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        if is_train:\n",
    "            df = self.train_df\n",
    "        else:\n",
    "            df = self.val_df\n",
    "            \n",
    "        if lang != '_all':\n",
    "            df = df[df['lang'] == lang]\n",
    "            \n",
    "        rows = df[df['question'].isin(questions)]\n",
    "        \n",
    "        def tokenize_by_lang(row):\n",
    "            if row['lang'] == 'ja':\n",
    "                return len(self.japanese_tokenizer.parse(row['question']))\n",
    "            else:\n",
    "                return len(word_tokenize(row['question'], language=self.lang_map.get(row['lang'], 'english')))\n",
    "        \n",
    "        question_tokens = rows.apply(tokenize_by_lang, axis=1)\n",
    "        \n",
    "        #context is always in english\n",
    "        context_tokens = rows['context'].apply(lambda x: len(word_tokenize(x, language='english')))\n",
    "        \n",
    "        return question_tokens.values, context_tokens.values\n",
    "        \n",
    "    def gen_scatter_plot(\n",
    "        self, \n",
    "        lang: Literal['ja', 'ru', 'fi', '_all'] = '_all'\n",
    "    ):\n",
    "        df = self.train_df\n",
    "        if lang != '_all':\n",
    "            df = df[df['lang'] == lang]\n",
    "            \n",
    "        question_tokens, context_tokens = self.prepare_data(\n",
    "            df['question'].tolist(), \n",
    "            is_train=True, \n",
    "            lang=lang\n",
    "        )\n",
    "\n",
    "        answerable = df['answerable'].tolist()\n",
    "        return px.scatter(\n",
    "            title=f\"Scatter plot of question and context tokens for {self.lang_map[lang] if lang != '_all' else 'all'} languages\",\n",
    "            x=question_tokens, \n",
    "            y=context_tokens, \n",
    "            color=answerable,\n",
    "            color_discrete_map={False: 'blue', True: 'red'},\n",
    "            labels={'x':'Question Tokens', 'y':'Context Tokens', 'color':'Answerable'}\n",
    "        )\n",
    "        \n",
    "    def gen_plot(\n",
    "        self, \n",
    "        lang: Literal['ja', 'ru', 'fi', '_all'] = '_all'\n",
    "    ):\n",
    "        df = self.train_df\n",
    "        if lang != '_all':\n",
    "            df = df[df['lang'] == lang]\n",
    "            \n",
    "        question_tokens, context_tokens = self.prepare_data(\n",
    "            df['question'].tolist(), \n",
    "            is_train=True, \n",
    "            lang=lang\n",
    "        )\n",
    "\n",
    "        ratio = question_tokens / context_tokens\n",
    "        return px.histogram(\n",
    "            ratio, \n",
    "            title=f\"Histogram of question to context token ratio for {self.lang_map[lang] if lang != '_all' else 'all'} languages\",\n",
    "            labels={'x':'Question to Context Token Ratio', 'y':'Count'},\n",
    "            color=df['answerable'].tolist(),\n",
    "            color_discrete_map={False: 'blue', True: 'red'}\n",
    "        )\n",
    "        \n",
    "    def classify(\n",
    "        self, \n",
    "        questions: List[str], \n",
    "        is_train: bool = True,\n",
    "        lang: Literal['ja', 'ru', 'fi', '_all'] = '_all'\n",
    "    ) -> np.ndarray:\n",
    "        question_tokens, context_tokens = self.prepare_data(questions, is_train, lang)\n",
    "        \n",
    "        return question_tokens < 15\n",
    "        \n",
    "\n",
    "    def evaluate(self, lang: Literal['ja', 'ru', 'fi', '_all'] = '_all'):\n",
    "        questions = self.val_df['question'].tolist()\n",
    "        preds = self.classify(questions, is_train=False, lang=lang)\n",
    "        if lang != '_all':\n",
    "            labels = self.val_df[self.val_df['lang'] == lang]['answerable'].tolist()\n",
    "        else:\n",
    "            labels = self.val_df['answerable'].tolist()\n",
    "            \n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        f1 = f1_score(labels, preds)\n",
    "        precision = precision_score(labels, preds)\n",
    "        recall = recall_score(labels, preds)\n",
    "        # Create a confusion matrix\n",
    "        cm = confusion_matrix(labels, preds)\n",
    "        \n",
    "        # Create a heatmap of the confusion matrix\n",
    "        fig = px.imshow(cm,\n",
    "                        labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n",
    "                        x=['Not Answerable', 'Answerable'],\n",
    "                        y=['Not Answerable', 'Answerable'],\n",
    "                        text_auto=True,\n",
    "                        color_continuous_scale='Blues')\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"Confusion Matrix for {self.lang_map[lang] if lang != '_all' else 'all'} languages\",\n",
    "            xaxis_title=\"Predicted\",\n",
    "            yaxis_title=\"Actual\"\n",
    "        )\n",
    "        \n",
    "        # Save the figure\n",
    "        path = f\"plots/week1_c_confusion_matrix_{lang}.png\"\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "        fig.write_image(path)\n",
    "        \n",
    "        return fig, {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "\n",
    "classifier = TokenCountClassifier(ds_train, ds_val)\n",
    "question = ds_train['question']\n",
    "print(classifier.evaluate())\n",
    "\n",
    "for lang in ['ja', 'ru', 'fi']:\n",
    "    \"\"\" fig = classifier.gen_scatter_plot(lang=lang)\n",
    "    fig.show()\n",
    "    path = f\"week1/plots/week1_c_scatter_{lang}.png\"\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    fig.write_image(path) \"\"\"\n",
    "    vals = classifier.evaluate(lang)\n",
    "    vals[0].show()\n",
    "    \n",
    "    \n",
    "    print(f\"{lang}: {vals[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val[ds_val['lang'] == 'fi']['answerable'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = ds_train['question'][0]\n",
    "row = ds_train[ds_train['question'] == question]\n",
    "question_embedding = row['question_embedding'].values[0]\n",
    "context = row['context'].values[0]\n",
    "type(question_embedding)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
